%% abtex2-modelo-artigo.tex, v-1.9.7 laurocesar
%% Copyright 2012-2018 by abnTeX2 group at http://www.abntex.net.br/ 
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%%   http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 2005/12/01 or later.
%%
%% This work has the LPPL maintenance status `maintained'.
%% 
%% The Current Maintainer of this work is the abnTeX2 team, led
%% by Lauro César Araujo. Further information are available on 
%% http://www.abntex.net.br/
%%
%% This work consists of the files abntex2-modelo-artigo.tex and
%% abntex2-modelo-references.bib
%%

% ------------------------------------------------------------------------
% ------------------------------------------------------------------------
% abnTeX2: Modelo de Artigo Acadêmico em conformidade com
% ABNT NBR 6022:2018: Informação e documentação - Artigo em publicação 
% periódica científica - Apresentação
% ------------------------------------------------------------------------
% ------------------------------------------------------------------------

\documentclass[
	% -- opções da classe memoir --
	article,			% indica que é um artigo acadêmico
	11pt,				% tamanho da fonte
	oneside,			% para impressão apenas no recto. Oposto a twoside
	a4paper,			% tamanho do papel. 
	% -- opções da classe abntex2 --
	%chapter=TITLE,		% títulos de capítulos convertidos em letras maiúsculas
	%section=TITLE,		% títulos de seções convertidos em letras maiúsculas
	%subsection=TITLE,	% títulos de subseções convertidos em letras maiúsculas
	%subsubsection=TITLE % títulos de subsubseções convertidos em letras maiúsculas
	% -- opções do pacote babel --
	english,			% idioma adicional para hifenização
	brazil,				% o último idioma é o principal do documento
	sumario=tradicional
	]{abntex2}


% ---
% PACOTES
% ---

% ---
% Pacotes fundamentais 
% ---
\usepackage{lmodern}			% Usa a fonte Latin Modern
\usepackage[T1]{fontenc}		% Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}		% Codificacao do documento (conversão automática dos acentos)
\usepackage{indentfirst}		% Indenta o primeiro parágrafo de cada seção.
\usepackage{nomencl} 			% Lista de simbolos
\usepackage{color}				% Controle das cores
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{microtype} 			% para melhorias de justificação
\usepackage{syntax}				% BNF Grammar definition
% ---
		
% ---
% Pacotes adicionais, usados apenas no âmbito do Modelo Canônico do abnteX2
% ---
\usepackage{lipsum}				% para geração de dummy text
% ---
		
% ---
% Pacotes de citações
% ---
\usepackage[brazilian,hyperpageref]{backref}	 % Paginas com as citações na bibl
\usepackage[alf]{abntex2cite}	% Citações padrão ABNT
% ---

% ---
% Configurações do pacote backref
% Usado sem a opção hyperpageref de backref
\renewcommand{\backrefpagesname}{Citado na(s) página(s):~}
% Texto padrão antes do número das páginas
\renewcommand{\backref}{}
% Define os textos da citação
\renewcommand*{\backrefalt}[4]{
	\ifcase #1 %
		Nenhuma citação no texto.%
	\or
		Citado na página #2.%
	\else
		Citado #1 vezes nas páginas #2.%
	\fi}%
% ---

% --- Informações de dados para CAPA e FOLHA DE ROSTO ---
\titulo{Implementação do Analisador Léxico}
\tituloestrangeiro{Implementation of the Lexical Analyzer}

\autor{
Mikael Mello\thanks{Departamento de Ciência da Computação, Universidade de Brasília,
Brasília, DF, Brasil.
\mbox{\href{mailto:contact@mikaelmello.com}{contact@mikaelmello.com}} }
}

\local{Brasil}
\data{Setembro 2019}
% ---

% ---
% Configurações de aparência do PDF final

% alterando o aspecto da cor azul
\definecolor{blue}{RGB}{41,5,195}

% informações do PDF
\makeatletter
\hypersetup{
     	%pagebackref=true,
		pdftitle={\@title}, 
		pdfauthor={\@author},
    	pdfsubject={},
	   pdfcreator={Mikael Mello},
		pdfkeywords={tradutores}{relatório}{escolha do tema}, 
		colorlinks=true,       		% false: boxed links; true: colored links
    	linkcolor=blue,          	% color of internal links
    	citecolor=blue,        		% color of links to bibliography
    	filecolor=magenta,      		% color of file links
		urlcolor=blue,
		bookmarksdepth=4
}
\makeatother
% --- 

% ---
% compila o indice
% ---
\makeindex
% ---

% ---
% Altera as margens padrões
% ---
\setlrmarginsandblock{3cm}{3cm}{*}
\setulmarginsandblock{3cm}{3cm}{*}
\checkandfixthelayout
% ---

% --- 
% Espaçamentos entre linhas e parágrafos 
% --- 

% O tamanho do parágrafo é dado por:
\setlength{\parindent}{1.3cm}

% Controle do espaçamento entre um parágrafo e outro:
\setlength{\parskip}{0.2cm}  % tente também \onelineskip

% Espaçamento simples
\SingleSpacing


% ----
% Início do documento
% ----
\begin{document}

% Seleciona o idioma do documento (conforme pacotes do babel)
%\selectlanguage{english}
\selectlanguage{brazil}

% Retira espaço extra obsoleto entre as frases.
\frenchspacing 

% ----------------------------------------------------------
% ELEMENTOS PRÉ-TEXTUAIS
% ----------------------------------------------------------

%---
%
% Se desejar escrever o artigo em duas colunas, descomente a linha abaixo
% e a linha com o texto ``FIM DE ARTIGO EM DUAS COLUNAS''.
% \twocolumn[    		% INICIO DE ARTIGO EM DUAS COLUNAS
%
%---

% página de titulo principal (obrigatório)
\maketitle


% titulo em outro idioma (opcional)



% resumo em português
\begin{resumoumacoluna}
	Neste relatório é descrita a implementação do analisador léxico
	para a linguagem Afth, gerado pela ferramente Flex com a adição
	de um gerenciador de erros auxiliar.

	\vspace{\onelineskip}
	 
	\noindent
	\textbf{Palavras-chave}: tradutores. analisador léxico. flex.
\end{resumoumacoluna}


% resumo em inglês
\renewcommand{\resumoname}{Abstract}
\begin{resumoumacoluna}
	\begin{otherlanguage*}{english}
		This report describes the implementation of the lexical
		analyzer for the Afth language, generated by the Flex tool
		along with the addition of a custom error handler.

		\vspace{\onelineskip}
		 
		\noindent
		\textbf{Keywords}: translators. lexical analyzer. flex.
	\end{otherlanguage*}  
\end{resumoumacoluna}

% ]  				% FIM DE ARTIGO EM DUAS COLUNAS
% ---

\begin{center}\smaller
	
	%   \textbf{Data de submissão e aprovação}: 26 de agosto de 2019.
	
\end{center}

% ----------------------------------------------------------
% ELEMENTOS TEXTUAIS
% ----------------------------------------------------------
\textual

\section{Introdução}

O analisador léxico é responsável por ler um texto de entrada,
normalmente de um arquivo, caractere por caractere de modo que
os agrupe em sequências chamadas lexemas. Estes lexemas têm
então um valor atribuído a eles e são convertidos em tokens. 
Estes tokens são a saída do programa, sendo normalmente lidos
pelo analisador sintático na próxima fase do processo de tradução.

Também é comum que o analisador léxico interaja com a tabela de
símbolos. Ao descobrir um lexema do tipo identificador, pode inserir
este lexema na tabela de símbolos.

O analisador léxico descrito neste relatório é limitado a apenas
escanear a entrada, identificar os tokens válidos e mostrá-los ao
usuário. Também é responsável por mostrar ao usuário todas as 
sequências de caracteres que não puderam ser agrupadas em lexemas. 

\section{Funcionamento do analisador}

O analisador léxico foi gerado pela ferramenta Flex
\cite{westes:flex}, uma ferramenta que gera programas que
reconhecem padrões léxicos em um texto. O arquivo
\texttt{src/tokenizer.lex} \cite{unb-translator-mikael} contém
todas as diretivas necessárias para que o Flex gere o \textit{scanner}
completo, sem modificações posteriores. 

Primeiramente, foram definidas as expressões regulares dos lexemas
que deveriam ser identificados pelo \textit{scanner}. São estes palavras
reservadas como \texttt{if}, \texttt{for}, \texttt{while},
\texttt{print}, entre outras, símbolos como parênteses
\texttt{'(', ')'}, colchetes \texttt{[ ]}, vírgula e ponto e
vírgula \texttt{',', ';'}, constantes como números, números com
casas decimais, cadeias de caracteres envolvidas por aspas duplas,
um caractere envolvido por aspas simples, entre outros. A lista
completa pode ser vista no código-fonte do arquivo
\texttt{src/tokenizer.lex}.

Após ter escrito todas as expressões regulares, bastou colocar
cada uma delas como um padrão a ser casado pelo \textit{scanner}.
Também foi adicionado ao código um enumerador com um número único
para cada tipo de lexema a ser identificado, de modo que ao chamar
a função \texttt{yylex()}, o \textit{scanner} trata de casar a maior
quantidade possível de caracteres em uma expressão regular de um
lexema e quando o consegue, retorna o identificador numérico deste
lexema para o invocador da função.

Assim, basta chamar continuamente \texttt{yylex()} até que não
haja mais caracteres a serem lidos, caso em que a função retornará
o valor 0. Deste modo, para cada chamada de função em que um lexema
é corretamente identificado, é impresso na tela o tipo do lexema
e a cadeia de caracteres que casaram com seu padrão.

O \textit{scanner} também implementa comentários, isto é, seções do código
que não devem ser escaneadas como sendo parte do programa, tendo
como único fim servir como anotações. Por isso, sempre que o
\textit{scanner} identifica a expressão regular \texttt{/*}, ignora todos
os caracteres até que encontre o padrão \texttt{*/}, sendo isto
definido como comentários em bloco. Também é implementado o comentário
em linha, onde quando se encontra a expressão \texttt{//}, todos
os caracteres são ignorados até que se encontre um caractere
de quebra de linha.

\subsection{Tratamento de erros}

O analisador léxico, ao identificar um lexema, o imprime e
continua a ler os próximos caracteres buscando casar com algum
padrão definido. Se não é possível casar o resto da entrada com
algum dos padrões dos lexemas, o analisador retorna o caractere
atual como um erro e tenta novamente a partir do próximo caractere.

Baseado nisso, quando vários caracteres contínuos não casam com
nenhum padrão, eles serão agrupados em apenas um erro, de modo 
a não poluir a saída para o usuário, mostrando um erro por caractere
inválido.

Para isto, foi criado um tipo de dados chamado \texttt{t_lexical_error},
erro léxico, que armazena a cadeia de caracteres contínuos de um erro,
além da linha e coluna do primeiro caractere pertencente à cadeia.

No início da análise léxica, mantém-se uma referência nula a um
objeto de erro, indicando que ainda não houveram caracteres sem
casamento de padrão.

Quando um caractere não é reconhecido e o lexema indicando erro é retornado
pelo \textit{scanner} e o objeto de erro é nulo, deve-se criar um novo
objeto de erro e apontar esta referência para ele. Este objeto tem como
elementos a cadeia de caracteres não reconhecida, que no momento
contém apenas o caractere atual, e sua posição, a do caractere atual. 

Se o objeto não for nulo, isto é, um erro já estiver sido iniciado,
este caractere é adicionado à cadeia do erro atual.

Se um padrão for reconhecido e um lexema válido for retornado
ou a leitura da entrada for finalizada, deve-se verificar se o objeto
de erro é nulo. Se não for nulo, isto significa que a sequência
de caracteres não reconhecidos foi terminada e então o objeto de
erro deve ser finalizado.

Para imprimir os erros apenas depois que todos os lexemas válidos
forem reconhecidos, os objetos de erro são armzenados em uma lista.
Após toda a entrada ser lida, realiza-se uma iteração sobre todos
os elementos da lista, de modo que cada erro dentro dela é mostrado
ao usuário, indicando a cadeia de caracteres não reconhecida e a
posição de seu início.

\subsection{Alterações feitas na gramática}

Nesta estapa do desenvolvimento do tradutor, foram adicionados
à gramática as regras relacionadas à entrada e saída, sendo criadas
as regras \textit{scan-words} e \textit{print-words}, definindo
as palavras reservadas para leitura e escrita de dados, assim como
as regras \textit{scan} e \textit{print}, que definem a sintaxe
de tais operações.

Além disso, \textit{var-declaration} e \textit{assignment} deixam
de ser produtos da regra \textit{expression} e são agora produtos
diretos da regra \textit{statement}. Isto deve-se ao fato de que
uma \textit{expression} poderia ser encerrada sem um ponto e vírgula
no final até quando fosse filha direta da regra \textit{statement}.
Deste modo foi necessário colocar um ponto e vírgula após \textit{expression}
como produto de \textit{statement}. Como ambas \textit{var-declaration}
e \textit{assignment} já terminam com ponto e vírgula, a gramática
exigiria dois, por isso foi necessário movê-las.

Também foi adicionada a palavra \texttt{void} como um dos tipos
de dados disponíveis, onde só deve ser usada como tipos de retorno
de funções.

\subsection{Testes}

Para testar a identificação correta de tokens foram escritos quatro
arquivos, de extensão \texttt{.afth}, dois deles apenas com tokens
válidos e dois com alguns erros.

O primeiro, \texttt{samples/correct.lexical.afth}, contém apenas
algumas declarações de variáveis e expressões aritméticas e condicionais
simples, juntamente com uma expressão de saída \texttt{print}.
Todos os tokens são corretamente identificados, as variáveis são
interpretadas como lexemas do tipo identificador, as palavras reservadas
são identificadas de forma apropriada assim como diversos símbolos.

O segundo, \texttt{samples/correct2.lexical.afth}, possui a declaração
de uma variável do tipo conjunto de inteiros, onde todos os tokens são
devidamente agrupados em lexemas apropriados, assim como nas operações
de inserção e remoção dos elementos neste conjunto. Os símbolos de
maior e menor duplos são devidamente agrupados como um só lexema, assim
como as palavras reservadas \texttt{in} e \texttt{rm}.

O terceiro, \texttt{samples/strange-chars.lexical.afth}, possui
cadeias de caracteres não presentes na gramática da linguagem. As três
são devidamente informadas como inválidas pelo analisador léxico,
indicando corretamente a linha e a coluna onde são iniciadas.

O quarto, \texttt{samples/identifier-invalid-char.lexica.afth}, possui
testes para verificar a análise correta de identificadores. Várias letras
contínuas, separadas por símbolos como \texttt{-} ou \texttt{/}, são
devidamente interpretados como diferentes identificadores, uma vez que
o erro presente é sintático, não são reportados erros pelo analisador léxico.
Além disso, um dos identificadores possui um caractere inválido no
meio da cadeia, o analisador interpreta corretamente cada metade da cadeia
como sendo um identificador diferente e o caractere inválido é identificado
como um erro e impresso no final da execução do analisador.

\section{Gramática da linguagem}


\setlength{\grammarparsep}{0pt plus 1pt minus 1pt} % increase separation between rules
\setlength{\grammarindent}{12em} % increase separation between LHS/RHS 

\begin{grammar}
	
	<program> ::= <declaration-list>
		
	<declaration-list> ::= <declaration-list> <declaration> | <declaration>
	
	<declaration> ::= <var-declaration> | <fun-declaration>
	
	<var-declaration> ::= <type> <identifier> `;'
	\alt <type> <identifier> `[' <integer> `]' `;'
	\alt <type> <identifier> `\{' `\}' `;'
	
	<fun-declaration> ::= <type> <identifier> `(' <params> `)' <scope>
		
	<params> ::= <params> `,' <param> | <param> | $\varepsilon$
	
	<param> ::= <type> <identifier>
	\alt <type> <identifier> `[' `]'
	\alt <type> <identifier> `\{' `\}'
	
	<scope> ::= `\{' <statement-list> `\}'
	
	<statement-list> ::= <statement-list> <statement> | $\varepsilon$
	
	<statement> ::= <scope>
	\alt <var-declaration>
	\alt <assignment>
	\alt <print>
	\alt <scan>
	\alt <expression> `;'
	\alt <condition>
	\alt <iteration>
	\alt <return>
	
	<print> ::= <print-words> <expression> `;'

	<scan> ::= <scan-words> <identifier> `;'

	<print-words> ::= `print' | `printc' | `printx'

	<scan-words> ::= `scan' | `scanc' | `scanf'	

	<condition> ::= `if' `(' <expression> `)' <statement> <condition-mid>
		
	<condition-mid> ::= `else if `(' <expression> `)' <statement> <condition-mid>
	\alt <condition-end>
	
	<condition-end> ::= `else' <statement> | $\varepsilon$
	
	<iteration> ::= `while' `(' <expression> `)' <statement>
	\alt `for' `(' <expression>? `;' <expression>? `;' <expression>? `)' <statement>
	
	<return> ::= `return' <expression>? `;'
	
	<assignment> ::= <identifier> <assignment-op> <expression> `;'
	
	<expression> ::= <and-expression>
	
	<and-expression> ::= <or-expression>
	\alt <and-expression> `&&' <or-expression>
	
	<or-expression> ::= <bw-or-expression>
	\alt <or-expression> `||' <bw-or-expression>
	
	<bw-or-expression> ::= <bw-xor-expression>
	\alt <bw-or-expression> `|' <bw-xor-expression>
	
	<bw-xor-expression> ::= <bw-and-expression>
	\alt <bw-xor-expression> `^' <bw-and-expression>
	
	<bw-and-expression> ::= <eq-expression>
	\alt <bw-and-expression> `&' <eq-expression>
	
	<eq-expression> ::= <relational-expression>
	\alt <eq-expression> `==' <rel-expression>
	\alt <eq-expression> `!=' <rel-expression>
	
	<rel-expression> ::= <shift-expression>
	\alt <rel-expression> <rel-op> <shift-expression>
	
	<shift-expression> ::= <add-expression>
	\alt <shift-expression> `<<' <add-expression>
	\alt <shift-expression> `>>' <add-expression>
	\alt <shift-expression> `rm' <add-expression>
	
	<add-expression> ::= <mult-expression>
	\alt <add-expression> `+' <mult-expression>
	\alt <add-expression> `-' <mult-expression>
	
	<mult-expression> ::= <cast-expression>
	\alt <mult-expression> <mul-op> <cast-expression>
	
	<cast-expression> ::= <unary-expression>
	\alt `(' <type> `)' <cast-expression>
	
	<unary-expression> ::= <postfix-expression>
	\alt <unary-op> <cast-expression>
	\alt sizeof <cast-expression>
	
	<postfix-expression> ::= <primary-expression>
	\alt <postfix-expression> [ <expression> ]
	\alt <postfix-expression> ( <param-values> )
		
	<param-values> ::= <param-values> `,' <expression> | <expression> | $\varepsilon$
	
	<primary-expression> ::= <identifier>
	\alt <constant>
	\alt <string>
	\alt `(' <expression> `)'
	
	<constant> ::= <integer>
	\alt `'' <symbol> `''
	\alt <integer> `.' <integer>
	
	<integer> ::= <digit>+
	
	<identifier> ::= <letter> \{ <letter> | <digit> | `_' \}*

	<type> ::= `void' | `bool' | `byte' | `char' | `short' | `int' | `long' | `float' | `double'
	
	<symbol> ::= any printable ascii character

	<letter> ::= `a' | `b' | ... | `z' | `A' | ... | `Z'
	
	<digit> ::= `0' | `1' | `2' | ... | `9'
	
	<assignment-op> ::= `=' | `+=' | `-=' | `*=' | `/=' | `\%='
	
	<unary-op> ::= `+' | `-' | `~' | `!'
	
	<rel-op> ::= `<' | `>' | `<=' | `>=' | `in'
	
	<mul-op> ::= `*' | `/' | `\%'
	
\end{grammar}

\section{Considerações finais}

A construção do analisador léxico torna-se extremamente simples com
o auxílio da ferramente Flex, uma vez que basta especificar os padrões
a serem casados, sem ser necessário se preocupar com a corretude
das funções que leem a entrada e são responsáveis por identificar
os lexemas.

Nesta fase, foi necessário desenvolver estruturas personalizadas
para facilitar o gerenciamento de erros, porém são estruturas simples,
assim a maior complexidade de desenvolver um analisador léxico
continua a ser o casamento de entrada com padrões pré-definidos, o que
é majoritariamente feito pelo Flex.

\bookmarksetup{startatroot}% 

% ----------------------------------------------------------
% ELEMENTOS PÓS-TEXTUAIS
% ----------------------------------------------------------
\postextual

% ----------------------------------------------------------
% Referências bibliográficas
% ----------------------------------------------------------
\bibliography{references}

% ----------------------------------------------------------
% Glossário
% ----------------------------------------------------------
%
% Há diversas soluções prontas para glossário em LaTeX. 
% Consulte o manual do abnTeX2 para obter sugestões.
%
%\glossary

% ----------------------------------------------------------
% Apêndices
% ----------------------------------------------------------

% ---
% Inicia os apêndices
% ---
% \begin{apendicesenv}

% % ----------------------------------------------------------
% \chapter{Nullam elementum urna vel imperdiet sodales elit ipsum pharetra ligula
% ac pretium ante justo a nulla curabitur tristique arcu eu metus}
% % ----------------------------------------------------------
% \lipsum[55-56]

% \end{apendicesenv}
% ---

% ----------------------------------------------------------
% Anexos
% ----------------------------------------------------------
% \cftinserthook{toc}{AAA}
% % ---
% % Inicia os anexos
% % ---
% %\anexos
% \begin{anexosenv}

% % ---
% \chapter{Cras non urna sed feugiat cum sociis natoque penatibus et magnis dis
% parturient montes nascetur ridiculus mus}
% % ---

% \lipsum[31]

% \end{anexosenv}

% ----------------------------------------------------------
% Agradecimentos
% ----------------------------------------------------------

% \section*{Agradecimentos}
% Texto sucinto aprovado pelo periódico em que será publicado. Último 
% elemento pós-textual.

\end{document}
